{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSIsoft(r) AF SDK Version: 3.0.2.5\n"
     ]
    }
   ],
   "source": [
    "#Librerias PI\n",
    "from PIconnect import PIData, PIServer, PIConfig\n",
    "\n",
    "# Librerias de Python\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones para DESCARGA DATOS PI OSISOFT AVEVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del servidor PI\n",
    "PIServer.DEFAULT_SERVER = 'uwgepi'  # Cambia por el nombre de tu servidor\n",
    "usuario = 'UF183530'  # Cambia por tu usuario\n",
    "contraseña = 'UF183530'  # Cambia por tu contraseña\n",
    "# Configurar la zona horaria predeterminada de PIconnect\n",
    "PIConfig.DEFAULT_TIMEZONE = 'Europe/Madrid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fecha inicial (yyyy, mm, dd, hh, mm, ss)\n",
    "fecha_inicio = datetime(2014, 1, 1, 0, 0, 0)  # Cambia por la fecha de inicio deseada\n",
    "# Fecha fin: se puede poner un delta de tiempo \n",
    "# fecha_fin =fecha_inicio + timedelta(days=20)\n",
    "fecha_fin = datetime(2025, 2, 1, 0, 0, 0)  # Cambia por el rango de tiempo deseado\n",
    "intervalo = \"10s\" # Formato PI\n",
    "# Definicion de tags a descargar\n",
    "tags = {\n",
    "    \"SAB:CBOP.A81.PAC11.AP001XH01\",\n",
    "    \"SAB:CBOP.A81.PAC13.AP001XH01\"\n",
    "}\n",
    "# Filtro para descarga. Dejar vacio sino se quiere filtro\n",
    "# Los tags deben ir entre ''\n",
    "#filtro = \"'SAB:G1.TNH_V'>2995\" \n",
    "filtro = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga datos de PI en carpeta \"data\" \n",
    "# Crea un fichero por cada mes del año\n",
    "\n",
    "# Conexión al servidor\n",
    "with PIServer() as server:\n",
    "    # Iterar de mes en mes entre las fechas especificadas\n",
    "    fecha_actual = fecha_inicio\n",
    "    while fecha_actual < fecha_fin:\n",
    "        # Calcular el final del mes actual\n",
    "        mes_siguiente = (fecha_actual.replace(day=28) + timedelta(days=4)).replace(day=1)\n",
    "        fecha_siguiente = min(mes_siguiente, fecha_fin)\n",
    "        \n",
    "        # Crear un DataFrame vacío para el mes actual\n",
    "        df_mes = pd.DataFrame()\n",
    "        \n",
    "        for tag in tags:\n",
    "            print(f\"Descargando datos para {tag} desde {fecha_actual} hasta {fecha_siguiente}...\")\n",
    "            # Buscar la etiqueta en el servidor\n",
    "            punto = server.search(tag)\n",
    "            if not punto:\n",
    "                print(f\"Advertencia: No se encontró la etiqueta {tag}\")\n",
    "                continue\n",
    "            \n",
    "            punto = punto[0]  # Toma el primer resultado de la búsqueda\n",
    "            \n",
    "            # Recuperar datos interpolados para el rango mensual\n",
    "            valores = punto.interpolated_values(fecha_actual, fecha_siguiente, intervalo, filtro)\n",
    "            \n",
    "            # Convertir a un DataFrame temporal para normalizar los datos\n",
    "            df_temp = pd.DataFrame(valores.items(), columns=[\"Timestamp\", tag])\n",
    "            df_temp.set_index(\"Timestamp\", inplace=True)\n",
    "            \n",
    "            # Unir los datos del tag actual al DataFrame del mes\n",
    "            if df_mes.empty:\n",
    "                df_mes = df_temp\n",
    "            else:\n",
    "                df_mes = df_mes.join(df_temp, how='outer')\n",
    "        \n",
    "        # Crear carpeta para el año si no existe\n",
    "        year_folder = os.path.join(\"datos\", str(fecha_actual.year))\n",
    "        os.makedirs(year_folder, exist_ok=True)\n",
    "        \n",
    "        # Guardar el DataFrame del mes en un archivo CSV\n",
    "        csv_filename = os.path.join(year_folder, f\"{fecha_actual.year}_{fecha_actual.month:02d}.csv\")\n",
    "        df_mes.to_csv(csv_filename, sep=';', decimal=',')\n",
    "        \n",
    "        # Avanzar al siguiente mes\n",
    "        fecha_actual = fecha_siguiente\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear dataframe de todos los CSV descargados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear dataframe\n",
    "def leer_csv_en_carpetas(ruta_base):\n",
    "    # Crear una lista para almacenar los DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    # Recorrer carpetas y subcarpetas con os.walk\n",
    "    for carpeta, subcarpetas, archivos in os.walk(ruta_base):\n",
    "        #print(f\"Explorando carpeta: {carpeta}\")  # Depuración\n",
    "        for archivo in archivos:\n",
    "            if archivo.endswith('.csv'):  # Filtrar solo archivos CSV\n",
    "                ruta_completa = os.path.join(carpeta, archivo)\n",
    "                #print(f\"Encontrado archivo: {ruta_completa}\")  # Depuración\n",
    "                try:\n",
    "                    # Leer el archivo CSV\n",
    "                    df = pd.read_csv(ruta_completa, sep=';', decimal=',')\n",
    "                    # Convierto la columna de fechas que viene como texto a formato datatime de pandas\n",
    "                    df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce', utc=True)\n",
    "                    dataframes.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al leer el archivo {ruta_completa}: {e}\")\n",
    "    \n",
    "    # Concatenar todos los DataFrames si hay datos\n",
    "    if dataframes:       \n",
    "        df_combinado = pd.concat(dataframes, ignore_index=True)\n",
    "        # Información básica\n",
    "        print (\"Tipo datos\")\n",
    "        print (df_combinado.info())\n",
    "        contar_valores_distintos = df_combinado.nunique() # Muestra cantidad valores unicos de cada señal\n",
    "        print (\"Valores distintos por columnas:\")\n",
    "        print (contar_valores_distintos)\n",
    "        return df_combinado\n",
    "    else:\n",
    "        print(\"No se encontraron archivos CSV en las subcarpetas.\")\n",
    "        return pd.DataFrame()  # Devuelve un DataFrame vacío si no hay datos\n",
    "    \n",
    "# Funcion para convertir tipo datos\n",
    "def convertir_columnas_a_numerico(df, columnas, tipo_dato = float):\n",
    "    \"\"\"\n",
    "    Convierte las columnas especificadas a datos numéricos, eliminando las filas\n",
    "    con valores no numéricos en esas columnas.\n",
    "\n",
    "    Parámetros:\n",
    "        df (pd.DataFrame): El dataframe a procesar.\n",
    "        columnas (list): Lista de nombres de columnas a convertir.\n",
    "\n",
    "    Retorna:\n",
    "        pd.DataFrame: El dataframe con las columnas convertidas y filas no numéricas eliminadas.\n",
    "    \"\"\"\n",
    "    # Aplicar la conversión a numérico con manejo de errores en las columnas especificadas\n",
    "    for columna in columnas:\n",
    "        df[columna] = pd.to_numeric(df[columna], errors='coerce')\n",
    "\n",
    "    # Eliminar filas con NaN en las columnas seleccionadas\n",
    "    df = df.dropna(subset=columnas)\n",
    "\n",
    "    # Convertir los datos al tipo especificado\n",
    "    df.loc[:, columnas] = df[columnas].astype(tipo_dato)\n",
    "\n",
    "    return df\n",
    "\n",
    "def EDA (df):\n",
    "    # Análsis exploratio de datos\n",
    "    sns.histplot(df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo datos\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5830704 entries, 0 to 5830703\n",
      "Data columns (total 3 columns):\n",
      " #   Column                        Dtype              \n",
      "---  ------                        -----              \n",
      " 0   Timestamp                     datetime64[ns, UTC]\n",
      " 1   SAB:CBOP.A81.PAC11.AP001XH01  object             \n",
      " 2   SAB:CBOP.A81.PAC13.AP001XH01  object             \n",
      "dtypes: datetime64[ns, UTC](1), object(2)\n",
      "memory usage: 133.5+ MB\n",
      "None\n",
      "Valores distintos por columnas:\n",
      "Timestamp                       5830571\n",
      "SAB:CBOP.A81.PAC11.AP001XH01         12\n",
      "SAB:CBOP.A81.PAC13.AP001XH01         12\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ruta de la carpeta base donde estan los ficheros CSV(cambiar por la tuya)\n",
    "ruta_base = 'datos_PACS'\n",
    "\n",
    "# Llamar a la función para crear el dataframe completo\n",
    "df_leido = leer_csv_en_carpetas(ruta_base)\n",
    "\n",
    "# Convierto a tipo numerico las columnas que necesito\n",
    "# En los datos vendrán valores tipo texto \"BAD\", \"Error\", ... que provienen de PI\n",
    "columnas_a_convertir = ['SAB:CBOP.A81.PAC11.AP001XH01']\n",
    "tipo_conversion = int\n",
    "df_leido = convertir_columnas_a_numerico (df_leido, columnas_a_convertir, tipo_conversion)\n",
    "columnas_a_convertir = ['SAB:CBOP.A81.PAC13.AP001XH01']\n",
    "tipo_conversion = int\n",
    "df_leido = convertir_columnas_a_numerico (df_leido, columnas_a_convertir, tipo_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis exploratorio de datos\n",
    "EDA (df_leido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contador horas y números arranque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "# dataframe: dataframe a mirar\n",
    "# tag: nombre señal a mirar\n",
    "# estado_marcha: se indica si marcha es un 1 o un 0\n",
    "# intervalo: frecuencia de muestreo\n",
    "\n",
    "def contar_horas_arranques(dataframe, tag, estado_marcha, intervalo):\n",
    "    # Asegurarse de que la columna existe\n",
    "    if tag not in dataframe.columns:\n",
    "        raise ValueError(\"El DataFrame no contiene la señal: \" + tag)\n",
    "    \n",
    "    # Calcular la suma total\n",
    "    suma_total = dataframe[tag].sum()\n",
    "    \n",
    "    # Contar cambios de señal de 0 a 1\n",
    "    # Alternative longer form\n",
    "    if estado_marcha == 1:\n",
    "        estado_marcha = 1\n",
    "        estado_paro = 0 \n",
    "    else:   \n",
    "        estado_marcha = 0\n",
    "        estado_paro = 1\n",
    "\n",
    "    cambios_estado = ((dataframe[tag] == estado_marcha) & (dataframe[tag].shift(1) == estado_paro)).sum()\n",
    "    \n",
    "    segundos_totales = suma_total * int(intervalo)\n",
    "    horas = segundos_totales // 3600\n",
    "    minutos = (segundos_totales % 3600) // 60\n",
    "    return {\n",
    "        'horas_arrancado': horas,\n",
    "        'minutos_arrancado': minutos,\n",
    "        'arranques': cambios_estado\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número arranques: 715\n",
      "Tiempo arrancado de SAB:CBOP.A81.PAC11.AP001XH01: 46037.0 horas y 19.0 minutos\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "# Primero hago una copia\n",
    "df_tratado = df_leido.copy()\n",
    "\n",
    "tag = \"SAB:CBOP.A81.PAC11.AP001XH01\"\n",
    "estado_marcha = 1\n",
    "intervalo_muestreo_datos = 60 # Ponerlo en segundos\n",
    "resultado = contar_horas_arranques(df_leido, tag, estado_marcha, intervalo_muestreo_datos)\n",
    "\n",
    "print(\"Número arranques:\", resultado['arranques'])\n",
    "# El tiempo arrancado lo \n",
    "print(f\"Tiempo arrancado de {tag}: {resultado['horas_arrancado']} horas y {resultado['minutos_arrancado']} minutos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Máximos y mínimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo fichero TAGS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884bd3e204bf4944985ad9604ba9c693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando tags:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: Sabón.Potencia Activa - Float64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8c54a514b14c43a806ed00e12f787b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando Sabón.Potencia Activa:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: SAB.Calidad_ABC - Float64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aadcf94c5e4bb0a3fa1f5f38b64155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando SAB.Calidad_ABC:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: SAB.ERM.Descripcion estado - String\n",
      "Procesando: SAB.ERM.Down - Int16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba05cc507d24436a966aab1ae7b6e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando SAB.ERM.Down:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: SAB.ERM.Fail - Int16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f000be6cf31648c1abd3eb88bcb10799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando SAB.ERM.Fail:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: SAB.ERM.Status - Int16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f162e0f9b2042c79945a4b59c537798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando SAB.ERM.Status:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: SAB.ERM.Substatus_Down - Int16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab1465307cd47ce8e6d356062653ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando SAB.ERM.Substatus_Down:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: SAB.ERM_FC1.Descripcion estado - String\n",
      "Procesando: SAB.ERM_FC1.Down - Int16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf9af1d3a014ee890bca589ce452ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando SAB.ERM_FC1.Down:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: SAB.ERM_FC1.Fail - Int16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3338b6d0244113a72699aa00c8a0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando SAB.ERM_FC1.Fail:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizado\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import PIconnect as PI\n",
    "from PIconnect.PIConsts import SummaryType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def leer_excel_a_dataframe(ruta_excel):\n",
    "    try:\n",
    "        df = pd.read_excel(ruta_excel)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {ruta_excel}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "fichero = \"señales_PI_SABON.xlsx\"\n",
    "print (\"Leyendo fichero TAGS\")\n",
    "tags_SABON = leer_excel_a_dataframe(fichero)\n",
    "cantidad_tags_a_leer = 10  # Este parámetro es para que lea solo los n primeros datos de la tabla de tags\n",
    "\n",
    "fecha_inicio = datetime(2024, 1, 1, 0, 0, 0)  # Cambia por la fecha de inicio deseada\n",
    "fecha_fin = datetime(2025, 2, 1, 0, 0, 0)  # Cambia por el rango de tiempo deseado\n",
    "\n",
    "with PI.PIServer() as server:\n",
    "    resultados = []\n",
    "    # Agregamos tqdm para mostrar el progreso\n",
    "    for index, row in tqdm(tags_SABON.head(cantidad_tags_a_leer).iterrows(), \n",
    "                           total=cantidad_tags_a_leer, desc=\"Procesando tags\", leave=True):\n",
    "        tag_name = row['Name']\n",
    "        tipo = row['pointtype']\n",
    "        print(f\"Procesando: {tag_name} - {tipo}\")\n",
    "        if tipo in ['Float64', 'Int16', 'Int32', 'Float32']:\n",
    "            try:\n",
    "                points = server.search(tag_name)[0]\n",
    "                fecha_actual = fecha_inicio\n",
    "                # Agregamos tqdm para mostrar el progreso mensual\n",
    "                total_meses = ((fecha_fin.year - fecha_inicio.year) * 12 + fecha_fin.month - fecha_inicio.month) + 1\n",
    "                with tqdm(total=total_meses-1, desc=f\"Procesando {tag_name}\", leave=False) as pbar:\n",
    "                    while fecha_actual < fecha_fin:\n",
    "                        mes_siguiente = (fecha_actual.replace(day=28) + timedelta(days=4)).replace(day=1)\n",
    "                        fecha_siguiente = min(mes_siguiente, fecha_fin)\n",
    "                        \n",
    "                        data = points.summary(fecha_actual, fecha_siguiente, SummaryType.MAXIMUM | SummaryType.MINIMUM)\n",
    "                        data['Tag'] = tag_name\n",
    "                        resultados.append(data)\n",
    "                        \n",
    "                        fecha_actual = fecha_siguiente\n",
    "                        pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al obtener datos para {tag_name}: {e}\")\n",
    "\n",
    "    df_resultados = pd.concat(resultados)\n",
    "\n",
    "    # Unificar los resultados por Tag\n",
    "    df_resultados_unificado = df_resultados.groupby('Tag').agg(\n",
    "        MINIMUM=('MINIMUM', 'min'),\n",
    "        MAXIMUM=('MAXIMUM', 'max'),\n",
    "        TIMESTAMP_MIN=('MINIMUM', 'idxmin'),\n",
    "        TIMESTAMP_MAX=('MAXIMUM', 'idxmax')\n",
    "    ).reset_index()\n",
    "\n",
    "    print(\"Finalizado\")\n",
    "    df_resultados_unificado.to_csv(\"tags_minimos_maximos.csv\", sep=\";\", decimal=\",\", encoding=\"utf-8-sig\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
