{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias PI\n",
    "from PIconnect import PIData, PIServer, PIConfig\n",
    "\n",
    "# Librerias de Python\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones para DESCARGA DATOS PI OSISOFT AVEVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del servidor PI\n",
    "PIServer.DEFAULT_SERVER = 'uwgepi'  # Cambia por el nombre de tu servidor\n",
    "usuario = 'UF183530'  # Cambia por tu usuario\n",
    "contraseña = 'UF183530'  # Cambia por tu contraseña\n",
    "# Configurar la zona horaria predeterminada de PIconnect\n",
    "PIConfig.DEFAULT_TIMEZONE = 'Europe/Madrid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fecha inicial (yyyy, mm, dd, hh, mm, ss)\n",
    "fecha_inicio = datetime(2014, 1, 1, 0, 0, 0)  # Cambia por la fecha de inicio deseada\n",
    "# Fecha fin: se puede poner un delta de tiempo \n",
    "# fecha_fin =fecha_inicio + timedelta(days=20)\n",
    "fecha_fin = datetime(2015, 1, 1, 0, 0, 0)  # Cambia por el rango de tiempo deseado\n",
    "intervalo = \"10s\" # Formato PI\n",
    "# Definicion de tags a descargar\n",
    "tags = {\n",
    "    \"SAB:CBOP.A81.PAC11.AP001XH01\",\n",
    "    \"SAB:CBOP.A81.PAC13.AP001XH01\"\n",
    "}\n",
    "# Filtro para descarga. Dejar vacio sino se quiere filtro\n",
    "# Los tags deben ir entre ''\n",
    "#filtro = \"'SAB:G1.TNH_V'>2995\" \n",
    "filtro = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga datos de PI en carpeta \"data\" \n",
    "# Crea un fichero por cada mes del año\n",
    "\n",
    "# Conexión al servidor\n",
    "with PIServer() as server:\n",
    "    # Iterar de mes en mes entre las fechas especificadas\n",
    "    fecha_actual = fecha_inicio\n",
    "    while fecha_actual < fecha_fin:\n",
    "        # Calcular el final del mes actual\n",
    "        mes_siguiente = (fecha_actual.replace(day=28) + timedelta(days=4)).replace(day=1)\n",
    "        fecha_siguiente = min(mes_siguiente, fecha_fin)\n",
    "        \n",
    "        # Crear un DataFrame vacío para el mes actual\n",
    "        df_mes = pd.DataFrame()\n",
    "        \n",
    "        for tag in tags:\n",
    "            print(f\"Descargando datos para {tag} desde {fecha_actual} hasta {fecha_siguiente}...\")\n",
    "            # Buscar la etiqueta en el servidor\n",
    "            punto = server.search(tag)\n",
    "            if not punto:\n",
    "                print(f\"Advertencia: No se encontró la etiqueta {tag}\")\n",
    "                continue\n",
    "            \n",
    "            punto = punto[0]  # Toma el primer resultado de la búsqueda\n",
    "            \n",
    "            # Recuperar datos interpolados para el rango mensual\n",
    "            valores = punto.interpolated_values(fecha_actual, fecha_siguiente, intervalo, filtro)\n",
    "            \n",
    "            # Convertir a un DataFrame temporal para normalizar los datos\n",
    "            df_temp = pd.DataFrame(valores.items(), columns=[\"Timestamp\", tag])\n",
    "            df_temp.set_index(\"Timestamp\", inplace=True)\n",
    "            \n",
    "            # Unir los datos del tag actual al DataFrame del mes\n",
    "            if df_mes.empty:\n",
    "                df_mes = df_temp\n",
    "            else:\n",
    "                df_mes = df_mes.join(df_temp, how='outer')\n",
    "        \n",
    "        # Crear carpeta para el año si no existe\n",
    "        year_folder = os.path.join(\"datos\", str(fecha_actual.year))\n",
    "        os.makedirs(year_folder, exist_ok=True)\n",
    "        \n",
    "        # Guardar el DataFrame del mes en un archivo CSV\n",
    "        csv_filename = os.path.join(year_folder, f\"{fecha_actual.year}_{fecha_actual.month:02d}.csv\")\n",
    "        df_mes.to_csv(csv_filename, sep=';', decimal=',')\n",
    "        \n",
    "        # Avanzar al siguiente mes\n",
    "        fecha_actual = fecha_siguiente\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear dataframe de todos los CSV descargados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear dataframe\n",
    "def leer_csv_en_carpetas(ruta_base):\n",
    "    # Crear una lista para almacenar los DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    # Recorrer carpetas y subcarpetas con os.walk\n",
    "    for carpeta, subcarpetas, archivos in os.walk(ruta_base):\n",
    "        #print(f\"Explorando carpeta: {carpeta}\")  # Depuración\n",
    "        for archivo in archivos:\n",
    "            if archivo.endswith('.csv'):  # Filtrar solo archivos CSV\n",
    "                ruta_completa = os.path.join(carpeta, archivo)\n",
    "                #print(f\"Encontrado archivo: {ruta_completa}\")  # Depuración\n",
    "                try:\n",
    "                    # Leer el archivo CSV\n",
    "                    df = pd.read_csv(ruta_completa, sep=';', decimal=',')\n",
    "                    # Convierto la columna de fechas que viene como texto a formato datatime de pandas\n",
    "                    df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce', utc=True)\n",
    "                    dataframes.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al leer el archivo {ruta_completa}: {e}\")\n",
    "    \n",
    "    # Concatenar todos los DataFrames si hay datos\n",
    "    if dataframes:       \n",
    "        df_combinado = pd.concat(dataframes, ignore_index=True)\n",
    "        # Información básica\n",
    "        print (\"Tipo datos\")\n",
    "        print (df_combinado.info())\n",
    "        contar_valores_distintos = df_combinado.nunique() # Muestra cantidad valores unicos de cada señal\n",
    "        print (\"Valores distintos por columnas:\")\n",
    "        print (contar_valores_distintos)\n",
    "        return df_combinado\n",
    "    else:\n",
    "        print(\"No se encontraron archivos CSV en las subcarpetas.\")\n",
    "        return pd.DataFrame()  # Devuelve un DataFrame vacío si no hay datos\n",
    "    \n",
    "# Funcion para convertir tipo datos\n",
    "def convertir_columnas_a_numerico(df, columnas, tipo_dato = float):\n",
    "    \"\"\"\n",
    "    Convierte las columnas especificadas a datos numéricos, eliminando las filas\n",
    "    con valores no numéricos en esas columnas.\n",
    "\n",
    "    Parámetros:\n",
    "        df (pd.DataFrame): El dataframe a procesar.\n",
    "        columnas (list): Lista de nombres de columnas a convertir.\n",
    "\n",
    "    Retorna:\n",
    "        pd.DataFrame: El dataframe con las columnas convertidas y filas no numéricas eliminadas.\n",
    "    \"\"\"\n",
    "    # Aplicar la conversión a numérico con manejo de errores en las columnas especificadas\n",
    "    for columna in columnas:\n",
    "        df[columna] = pd.to_numeric(df[columna], errors='coerce')\n",
    "\n",
    "    # Eliminar filas con NaN en las columnas seleccionadas\n",
    "    df = df.dropna(subset=columnas)\n",
    "\n",
    "    # Convertir los datos al tipo especificado\n",
    "    df.loc[:, columnas] = df[columnas].astype(tipo_dato)\n",
    "\n",
    "    return df\n",
    "\n",
    "def EDA (df):\n",
    "    # Análsis exploratio de datos\n",
    "    sns.histplot(df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta base donde estan los ficheros CSV(cambiar por la tuya)\n",
    "ruta_base = 'datos_arranque_PAC13'\n",
    "\n",
    "# Llamar a la función para crear el dataframe completo\n",
    "df_leido = leer_csv_en_carpetas(ruta_base)\n",
    "\n",
    "# Convierto a tipo numerico las columnas que necesito\n",
    "# En los datos vendrán valores tipo texto \"BAD\", \"Error\", ... que provienen de PI\n",
    "columnas_a_convertir = ['SAB:CBOP.A81.PAC13.AP001XH01']\n",
    "tipo_conversion = int\n",
    "df_leido = convertir_columnas_a_numerico (df_leido, columnas_a_convertir, tipo_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis exploratorio de datos\n",
    "EDA (df_leido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contador horas y números arranque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "# dataframe: dataframe a mirar\n",
    "# tag: nombre señal a mirar\n",
    "# estado_marcha: se indica si marcha es un 1 o un 0\n",
    "# intervalo: frecuencia de muestreo\n",
    "\n",
    "def contar_horas_arranques(dataframe, tag, estado_marcha, intervalo):\n",
    "    # Asegurarse de que la columna existe\n",
    "    if tag not in dataframe.columns:\n",
    "        raise ValueError(\"El DataFrame no contiene la señal: \" + tag)\n",
    "    \n",
    "    # Calcular la suma total\n",
    "    suma_total = dataframe[tag].sum()\n",
    "    \n",
    "    # Contar cambios de señal de 0 a 1\n",
    "    # Alternative longer form\n",
    "    if estado_marcha == 1:\n",
    "        estado_marcha = 1\n",
    "        estado_paro = 0 \n",
    "    else:   \n",
    "        estado_marcha = 0\n",
    "        estado_paro = 1\n",
    "\n",
    "    cambios_estado = ((dataframe[tag] == estado_marcha) & (dataframe[tag].shift(1) == estado_paro)).sum()\n",
    "    \n",
    "    segundos_totales = suma_total * int(intervalo)\n",
    "    horas = segundos_totales // 3600\n",
    "    minutos = (segundos_totales % 3600) // 60\n",
    "    return {\n",
    "        'horas_arrancado': horas,\n",
    "        'minutos_arrancado': minutos,\n",
    "        'arranques': cambios_estado\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso\n",
    "# Primero hago una copia\n",
    "df_tratado = df_leido.copy()\n",
    "\n",
    "tag = \"SAB:CBOP.A81.PAC13.AP001XH01\"\n",
    "estado_marcha = 1\n",
    "intervalo_muestreo_datos = 60 # Ponerlo en segundos\n",
    "resultado = contar_horas_arranques(df_leido, tag, estado_marcha, intervalo_muestreo_datos)\n",
    "\n",
    "print(\"Número arranques:\", resultado['arranques'])\n",
    "# El tiempo arrancado lo \n",
    "print(f\"Tiempo arrancado de {tag}: {resultado['horas_arrancado']} horas y {resultado['minutos_arrancado']} minutos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
